{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import OrderedDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed random number generation\n",
    "torch.manual_seed(66)\n",
    "np.random.seed(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Model\n",
    "### Fake Deep - CMS 155\n",
    "I will first import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIPS_NAME</th>\n",
       "      <th>SOURCE_REPORTING_UNIT_NAME</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>38.205000</td>\n",
       "      <td>-120.335000</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>215</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33.813100</td>\n",
       "      <td>-85.104300</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>82</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>32.201000</td>\n",
       "      <td>-82.498700</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>130</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32.509300</td>\n",
       "      <td>-81.708600</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>19</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>33.663889</td>\n",
       "      <td>-116.171944</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>215</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   LATITUDE   LONGITUDE  DISCOVERY_TIME  FIRE_SIZE  FIPS_NAME  \\\n",
       "0   0  38.205000 -120.335000           130.0       0.10        215   \n",
       "1   1  33.813100  -85.104300          1115.0       1.17         82   \n",
       "2   2  32.201000  -82.498700          1600.0       0.07        130   \n",
       "3   3  32.509300  -81.708600          1215.0       4.40         19   \n",
       "4   4  33.663889 -116.171944            -1.0       0.20        215   \n",
       "\n",
       "   SOURCE_REPORTING_UNIT_NAME  DATE  LABEL  \n",
       "0                         157     0      1  \n",
       "1                          71     0      4  \n",
       "2                          71     0      2  \n",
       "3                          71     0      4  \n",
       "4                          14     0      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./datasets/jeh_train_label-enc.csv\")\n",
    "# Rather than do real imputation,I just replace nans with zeros\n",
    "df_train = df_train.fillna(-1)\n",
    "df_train = df_train.drop(columns = [\"STATE\", \"FIPS_CODE\"])\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIPS_NAME</th>\n",
       "      <th>SOURCE_REPORTING_UNIT_NAME</th>\n",
       "      <th>DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>285382</td>\n",
       "      <td>34.346944</td>\n",
       "      <td>-117.442222</td>\n",
       "      <td>1605.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>158</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285383</td>\n",
       "      <td>34.020390</td>\n",
       "      <td>-116.179970</td>\n",
       "      <td>1545.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>218</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285384</td>\n",
       "      <td>38.068611</td>\n",
       "      <td>-120.276667</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>196</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285385</td>\n",
       "      <td>32.499971</td>\n",
       "      <td>-83.742573</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>87</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>285386</td>\n",
       "      <td>32.924940</td>\n",
       "      <td>-114.992530</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>89</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   LATITUDE   LONGITUDE  DISCOVERY_TIME  FIRE_SIZE  FIPS_NAME  \\\n",
       "0  285382  34.346944 -117.442222          1605.0        0.2        158   \n",
       "1  285383  34.020390 -116.179970          1545.0        0.1        218   \n",
       "2  285384  38.068611 -120.276667          1200.0        0.1        196   \n",
       "3  285385  32.499971  -83.742573            -1.0        0.4         87   \n",
       "4  285386  32.924940 -114.992530           126.0        0.1         89   \n",
       "\n",
       "   SOURCE_REPORTING_UNIT_NAME  DATE  \n",
       "0                         145     0  \n",
       "1                          69     0  \n",
       "2                         170     0  \n",
       "3                          47     1  \n",
       "4                          18     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"./datasets/jeh_test_label-enc.csv\")\n",
    "# df_test[\"Unnamed: 0\"] = df_test[\"Unnamed: 0\"].values + 285382\n",
    "df_test = df_test.fillna(-1)\n",
    "df_test = df_test.drop(columns = [\"STATE\", \"FIPS_CODE\"])\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I separate the data randomly into training and testing sets, with a 75/25 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(df_train.columns[1:-1])\n",
    "N_total = len(df_train)\n",
    "N = int(0.75 * N_total)\n",
    "N_test = N_total - N\n",
    "X_predict = df_test.to_numpy(dtype = float)[:, 1:]\n",
    "try:\n",
    "    X_train = np.load(\"./jeh_checkpoints/x_train.npy\")\n",
    "    Y_train = np.load(\"./jeh_checkpoints/y_train.npy\")\n",
    "    X_test = np.load(\"./jeh_checkpoints/x_test.npy\")\n",
    "    Y_test = np.load(\"./jeh_checkpoints/y_test.npy\")\n",
    "except FileNotFoundError:\n",
    "    test_indices = np.random.choice(list(range(N_total)), size=N_test, replace=False)\n",
    "    X_train = np.zeros([N, D], dtype = float)\n",
    "    Y_train = np.zeros(N, dtype = int)\n",
    "    X_test = np.zeros([N_test, D], dtype = float)\n",
    "    Y_test = np.zeros(N_test, dtype = int)\n",
    "    j = 0\n",
    "    k = 0\n",
    "    for i in range(len(df_train)):\n",
    "        if i not in test_indices:\n",
    "            X_train[j, :] = df_train.iloc[i, 1:-1]\n",
    "            Y_train[j] = df_train.iloc[i, -1]\n",
    "            j += 1\n",
    "        else:\n",
    "            X_test[k, :] = df_train.iloc[i, 1:-1]\n",
    "            Y_test[k] = df_train.iloc[i, -1]\n",
    "            k += 1\n",
    "    np.save(\"./jeh_checkpoints/x_train.npy\", X_train)\n",
    "    np.save(\"./jeh_checkpoints/y_train.npy\", Y_train)\n",
    "    np.save(\"./jeh_checkpoints/x_test.npy\", X_test)\n",
    "    np.save(\"./jeh_checkpoints/y_test.npy\", Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will apply a basic normalization to each column. I am careful to normalize everything using only the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(D):\n",
    "    mu = np.mean(X_train[:, i])\n",
    "    stddev = np.std(X_train[:, i])\n",
    "    X_train[:, i] = (X_train[:, i] - mu ) / stddev\n",
    "    X_test[:, i] = (X_test[:, i] - mu ) / stddev\n",
    "    X_predict[:, i] = (X_predict[:, i] - mu ) / stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We require Y_train and Y_test to be from 0-3, not 1-4\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I actually need to onehot encode the labels to the data set. In effect, my neural net will have 4 output units and I want the labels to emulate this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = len(np.unique(Y_train))\n",
    "Y_train_oh = np.zeros([len(Y_train), C])\n",
    "Y_test_oh = np.zeros([len(Y_test), C])\n",
    "for i in range(len(Y_train)):\n",
    "    y = Y_train[i]\n",
    "    Y_train_oh[i, y] = 1\n",
    "for i in range(len(Y_test)):\n",
    "    y = Y_test[i]\n",
    "    Y_test_oh[i, y] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to write a dataset class in order to use this set with pytorch. This is totally barebones, but I don't need to worry about streaming the dataset off the hard drive to multiple cores, since I have the memory to just store the entire dataset on each core. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"Dataset object for pytorch.\"\n",
    "    def __init__(self, X, Y):\n",
    "        'Initialization'\n",
    "        self.Y = Y.astype(float)\n",
    "        self.X = X.astype(float)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        x = self.X[index]\n",
    "        y = self.Y[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this class to actually construct dataset objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train, Y_train_oh)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use my GPU to try and speed up the neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "# When you are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will import some helper functions that I wrote in problem set 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    loss_val = 1\n",
    "    epoch = 0\n",
    "    while loss_val > 0.06:\n",
    "        epoch += 1\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Erase accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data.float())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(output, target.float())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight update\n",
    "            optimizer.step()\n",
    "        \n",
    "        loss_val = loss.item()\n",
    "        # Track loss each epoch\n",
    "        print('Train Epoch: %d  Loss: %.4f' % (epoch,  loss_val))\n",
    "\n",
    "def get_train_err(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    train_error = 0\n",
    "    train_loss = 0\n",
    "    # Turning off automatic differentiation\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data.float())\n",
    "            train_loss += loss_fn(output, target.float()).item() * len(target) # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=False).cpu().numpy()  # Get the index of the max class score\n",
    "            \n",
    "            # Convert the target back from onehot encoding\n",
    "            target = target.cpu().numpy()\n",
    "            target = target[:, 1] + target[:, 2] * 2 + target[:, 3] * 3\n",
    "            \n",
    "            # Determine the accuracy of the classification\n",
    "            correct += np.sum(pred==target)\n",
    "            temp = roc_auc_score(target, \n",
    "                                 output.cpu(), \n",
    "                                 multi_class='ovr',\n",
    "                                 labels=[0, 1, 2, 3]) \n",
    "            train_error += temp * len(target)\n",
    "            \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_error /= len(train_loader.dataset)\n",
    "    print(\"Average Training ROC AUC: %.3f\"%train_error)\n",
    "    print('Training set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "          (train_loss, correct, len(train_loader.dataset),\n",
    "           100. * correct / len(train_loader.dataset)))\n",
    "    \n",
    "def get_test_err(model):\n",
    "    # Putting layers like Dropout into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_error = 0\n",
    "    \n",
    "    # Turning off automatic differentiation\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data.float())\n",
    "            test_loss += loss_fn(output, target.float()).item() * len(target)  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=False).cpu().numpy()  # Get the index of the max class score\n",
    "            \n",
    "            # Convert the target back from onehot encoding\n",
    "            target = target.cpu().numpy()\n",
    "            target = target[:, 1] + target[:, 2] * 2 + target[:, 3] * 3\n",
    "            \n",
    "            # Determine the accuracy of the classification\n",
    "            correct += np.sum(pred==target)\n",
    "            test_error += roc_auc_score(target, \n",
    "                                        output.cpu(), \n",
    "                                        multi_class='ovr',\n",
    "                                        labels=[0, 1, 2, 3]) * len(target)\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_error /= len(test_loader.dataset)\n",
    "    print(\"Average Testing ROC AUC: %.3f\"%test_error)\n",
    "    print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "          (test_loss, correct, len(test_loader.dataset),\n",
    "           100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now try constructing and testing an **even deeper** neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (start): Linear(in_features=7, out_features=100, bias=True)\n",
      "  (expand1): Linear(in_features=100, out_features=1000, bias=True)\n",
      "  (expand3): Dropout(p=0.05, inplace=False)\n",
      "  (1-0): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (2-0): ReLU()\n",
      "  (3-0): Dropout(p=0.05, inplace=False)\n",
      "  (1-1): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (2-1): ReLU()\n",
      "  (3-1): Dropout(p=0.05, inplace=False)\n",
      "  (1-2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (2-2): ReLU()\n",
      "  (3-2): Dropout(p=0.05, inplace=False)\n",
      "  (1-3): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (2-3): ReLU()\n",
      "  (3-3): Dropout(p=0.05, inplace=False)\n",
      "  (1-4): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (2-4): ReLU()\n",
      "  (3-4): Dropout(p=0.05, inplace=False)\n",
      "  (1-5): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (2-5): ReLU()\n",
      "  (3-5): Dropout(p=0.05, inplace=False)\n",
      "  (1-6): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (2-6): ReLU()\n",
      "  (3-6): Dropout(p=0.05, inplace=False)\n",
      "  (1-7): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (2-7): ReLU()\n",
      "  (3-7): Dropout(p=0.05, inplace=False)\n",
      "  (1-8): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (2-8): ReLU()\n",
      "  (3-8): Dropout(p=0.05, inplace=False)\n",
      "  (1-9): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (2-9): ReLU()\n",
      "  (3-9): Dropout(p=0.05, inplace=False)\n",
      "  (narrow1): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (narrow2): ReLU()\n",
      "  (narrow3): Dropout(p=0.05, inplace=False)\n",
      "  (final-1): Linear(in_features=100, out_features=4, bias=True)\n",
      "  (final-2): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "height = 1000\n",
    "narrow = 100\n",
    "drop = 0.05\n",
    "depth = 10\n",
    "ordict = OrderedDict()\n",
    "ordict['start'] = nn.Linear(D, narrow)\n",
    "ordict['expand1'] = nn.Linear(narrow, height)\n",
    "ordict['expand3'] = nn.Dropout(drop)\n",
    "\n",
    "# Construct the bulk of the net\n",
    "for i in range(depth):\n",
    "    ordict['1-%i'%i] = nn.Linear(height, height)\n",
    "    ordict['2-%i'%i] = nn.ReLU()\n",
    "    ordict['3-%i'%i] = nn.Dropout(drop)\n",
    "    \n",
    "# Narrow the net and bring it down to the last few nodes\n",
    "ordict['narrow1'] = nn.Linear(height, narrow)\n",
    "ordict['narrow2'] = nn.ReLU()\n",
    "ordict['narrow3'] = nn.Dropout(drop)\n",
    "ordict['final-1'] = nn.Linear(narrow, C)\n",
    "ordict['final-2'] = nn.Softmax(dim=1)\n",
    "\n",
    "# Pack all the layers into the model\n",
    "model = nn.Sequential(ordict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(1e-4))\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will train the model and record the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.0827\n",
      "Train Epoch: 2  Loss: 0.0802\n",
      "Train Epoch: 3  Loss: 0.0849\n",
      "Train Epoch: 4  Loss: 0.0524\n",
      "Train Epoch: 5  Loss: 0.0609\n",
      "Train Epoch: 6  Loss: 0.0725\n",
      "Train Epoch: 7  Loss: 0.0646\n",
      "Train Epoch: 8  Loss: 0.0531\n",
      "Train Epoch: 9  Loss: 0.0631\n",
      "Train Epoch: 10  Loss: 0.0597\n",
      "Train Epoch: 11  Loss: 0.0588\n",
      "Train Epoch: 12  Loss: 0.0635\n",
      "Train Epoch: 13  Loss: 0.0591\n",
      "Train Epoch: 14  Loss: 0.0533\n",
      "Train Epoch: 15  Loss: 0.0608\n",
      "Train Epoch: 16  Loss: 0.0463\n",
      "Average Training ROC AUC: 0.785\n",
      "Training set: Average loss: 0.0617, Accuracy: 136308/214036 (63.6846)\n"
     ]
    }
   ],
   "source": [
    "train_model(model)\n",
    "get_train_err(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Testing ROC AUC: 0.779\n",
      "Test set: Average loss: 0.0622, Accuracy: 45279/71346 (63.4640)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Dataset(X_test, Y_test_oh)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=True)\n",
    "get_test_err(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the testing error like I will have to later. This line is mostly here because I was having trouble with it further down the line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.778895960705726"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "pred_max = np.zeros(len(X_test), dtype=int)\n",
    "pred_data = np.zeros([len(X_test), 5])\n",
    "test_dataset = Dataset(X_test, np.zeros(len(X_test)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        num = len(data)\n",
    "        output = model(data.float())\n",
    "        pred_data[i:i+num, 1:] = output.cpu().numpy()\n",
    "        pred_max[i:i+num] = output.argmax(dim=1, keepdim=False).cpu().numpy()\n",
    "        i += num\n",
    "\n",
    "roc_auc_score(Y_test, pred_data[:, 1:], multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model looks good. Let's train it on everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.0588\n"
     ]
    }
   ],
   "source": [
    "Y_oh = np.zeros([N_total, C])\n",
    "X = np.zeros([N_total, D])\n",
    "Y_oh[:N, :] = Y_train_oh\n",
    "Y_oh[N:, :] = Y_test_oh\n",
    "X[:N, :] = X_train\n",
    "X[N:, :] = X_test\n",
    "train_dataset = Dataset(X, Y_oh)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# Redefine the model\n",
    "model = nn.Sequential(ordict)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(1e-4))\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "# Retrain the model on all avaliable data and get the training error. \n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training ROC AUC: 0.802\n",
      "Training set: Average loss: 0.0598, Accuracy: 185120/285382 (64.8674)\n"
     ]
    }
   ],
   "source": [
    "get_train_err(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is well trained, I will predict the labels and submit to kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>285382</td>\n",
       "      <td>0.044244</td>\n",
       "      <td>0.527328</td>\n",
       "      <td>0.145882</td>\n",
       "      <td>0.282546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285383</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.442274</td>\n",
       "      <td>0.062724</td>\n",
       "      <td>0.225471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285384</td>\n",
       "      <td>0.701664</td>\n",
       "      <td>0.201726</td>\n",
       "      <td>0.022038</td>\n",
       "      <td>0.074573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285385</td>\n",
       "      <td>0.021629</td>\n",
       "      <td>0.397439</td>\n",
       "      <td>0.534408</td>\n",
       "      <td>0.046525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>285386</td>\n",
       "      <td>0.024907</td>\n",
       "      <td>0.435079</td>\n",
       "      <td>0.189782</td>\n",
       "      <td>0.350231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73030</th>\n",
       "      <td>358412</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>0.798848</td>\n",
       "      <td>0.097582</td>\n",
       "      <td>0.086614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73031</th>\n",
       "      <td>358413</td>\n",
       "      <td>0.014137</td>\n",
       "      <td>0.315036</td>\n",
       "      <td>0.106180</td>\n",
       "      <td>0.564646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73032</th>\n",
       "      <td>358414</td>\n",
       "      <td>0.061608</td>\n",
       "      <td>0.433696</td>\n",
       "      <td>0.089376</td>\n",
       "      <td>0.415319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73033</th>\n",
       "      <td>358415</td>\n",
       "      <td>0.065398</td>\n",
       "      <td>0.449002</td>\n",
       "      <td>0.123342</td>\n",
       "      <td>0.362258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73034</th>\n",
       "      <td>358416</td>\n",
       "      <td>0.044966</td>\n",
       "      <td>0.383328</td>\n",
       "      <td>0.162598</td>\n",
       "      <td>0.409108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73035 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        P1        P2        P3        P4\n",
       "0      285382  0.044244  0.527328  0.145882  0.282546\n",
       "1      285383  0.269531  0.442274  0.062724  0.225471\n",
       "2      285384  0.701664  0.201726  0.022038  0.074573\n",
       "3      285385  0.021629  0.397439  0.534408  0.046525\n",
       "4      285386  0.024907  0.435079  0.189782  0.350231\n",
       "...       ...       ...       ...       ...       ...\n",
       "73030  358412  0.016956  0.798848  0.097582  0.086614\n",
       "73031  358413  0.014137  0.315036  0.106180  0.564646\n",
       "73032  358414  0.061608  0.433696  0.089376  0.415319\n",
       "73033  358415  0.065398  0.449002  0.123342  0.362258\n",
       "73034  358416  0.044966  0.383328  0.162598  0.409108\n",
       "\n",
       "[73035 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "pred_data = np.zeros([len(X_predict), 5])\n",
    "test_dataset = Dataset(X_predict, np.zeros(len(X_predict)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, _ = data.to(device), target.to(device)\n",
    "        num = len(data)\n",
    "        output = model(data.float())\n",
    "        pred_data[i:i+num, 1:] = output.cpu().numpy()\n",
    "        i += num\n",
    "# append the id to each of the predictions\n",
    "pred_data[:, 0] = df_test[\"id\"].values\n",
    "pred_df = pd.DataFrame(pred_data, columns = [\"id\", \"P1\", \"P2\", \"P3\", \"P4\"])\n",
    "pred_df = pred_df.astype({'id': 'int'})\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"./FakeDeep_Attempt8_NeuralNet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNEcboXAHDpYYd97FWOW++h",
   "name": "Untitled"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
