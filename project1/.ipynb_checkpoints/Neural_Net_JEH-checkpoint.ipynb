{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import OrderedDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed random number generation\n",
    "torch.manual_seed(62)\n",
    "np.random.seed(62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Model\n",
    "### Fake Deep - CMS 155\n",
    "I will first import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIPS_NAME</th>\n",
       "      <th>FIPS_CODE</th>\n",
       "      <th>SOURCE_REPORTING_UNIT_NAME</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>33.813100</td>\n",
       "      <td>-85.104300</td>\n",
       "      <td>1</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>82</td>\n",
       "      <td>143.0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>32.201000</td>\n",
       "      <td>-82.498700</td>\n",
       "      <td>1</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>129</td>\n",
       "      <td>209.0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>32.509300</td>\n",
       "      <td>-81.708600</td>\n",
       "      <td>1</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>19</td>\n",
       "      <td>31.0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33.663889</td>\n",
       "      <td>-116.171944</td>\n",
       "      <td>0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>153</td>\n",
       "      <td>65.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33.166700</td>\n",
       "      <td>-116.634200</td>\n",
       "      <td>0</td>\n",
       "      <td>1330.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>153</td>\n",
       "      <td>65.0</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   LATITUDE   LONGITUDE  STATE  DISCOVERY_TIME  FIRE_SIZE  FIPS_NAME  \\\n",
       "0   1  33.813100  -85.104300      1          1115.0       1.17         82   \n",
       "1   2  32.201000  -82.498700      1          1600.0       0.07        129   \n",
       "2   3  32.509300  -81.708600      1          1215.0       4.40         19   \n",
       "3   4  33.663889 -116.171944      0          1400.0       0.20        153   \n",
       "4   5  33.166700 -116.634200      0          1330.0       5.00        153   \n",
       "\n",
       "   FIPS_CODE  SOURCE_REPORTING_UNIT_NAME  DATE  LABEL  \n",
       "0      143.0                          65     0      4  \n",
       "1      209.0                          65     0      2  \n",
       "2       31.0                          65     0      4  \n",
       "3       65.0                          13     0      2  \n",
       "4       65.0                         140     0      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./datasets/train_freq-imp_iqr-outliers_label-enc.zip\")\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIPS_NAME</th>\n",
       "      <th>FIPS_CODE</th>\n",
       "      <th>SOURCE_REPORTING_UNIT_NAME</th>\n",
       "      <th>DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>285382</td>\n",
       "      <td>34.346944</td>\n",
       "      <td>-117.442222</td>\n",
       "      <td>0</td>\n",
       "      <td>1605.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>158</td>\n",
       "      <td>71.0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285383</td>\n",
       "      <td>34.020390</td>\n",
       "      <td>-116.179970</td>\n",
       "      <td>0</td>\n",
       "      <td>1545.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>154</td>\n",
       "      <td>65.0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285384</td>\n",
       "      <td>38.068611</td>\n",
       "      <td>-120.276667</td>\n",
       "      <td>0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>196</td>\n",
       "      <td>109.0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285385</td>\n",
       "      <td>32.499971</td>\n",
       "      <td>-83.742573</td>\n",
       "      <td>1</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>87</td>\n",
       "      <td>153.0</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>285386</td>\n",
       "      <td>32.924940</td>\n",
       "      <td>-114.992530</td>\n",
       "      <td>0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>89</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   LATITUDE   LONGITUDE  STATE  DISCOVERY_TIME  FIRE_SIZE  FIPS_NAME  \\\n",
       "0  285382  34.346944 -117.442222      0          1605.0        0.2        158   \n",
       "1  285383  34.020390 -116.179970      0          1545.0        0.1        154   \n",
       "2  285384  38.068611 -120.276667      0          1200.0        0.1        196   \n",
       "3  285385  32.499971  -83.742573      1          1500.0        0.4         87   \n",
       "4  285386  32.924940 -114.992530      0           126.0        0.1         89   \n",
       "\n",
       "   FIPS_CODE  SOURCE_REPORTING_UNIT_NAME  DATE  \n",
       "0       71.0                         145     0  \n",
       "1       65.0                          69     0  \n",
       "2      109.0                         170     0  \n",
       "3      153.0                          47     1  \n",
       "4       25.0                          18     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"./datasets/test_freq-imp_label-enc.zip\")\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I separate the data randomly into training and testing sets, with a 75/25 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(df_train.columns[1:-1])\n",
    "N_total = len(df_train)\n",
    "N = int(0.75 * N_total)\n",
    "N_test = N_total - N\n",
    "train_indices = np.random.choice(list(range(N_total)), size=N, replace=False)\n",
    "X_predict = df_test.to_numpy(dtype = float)[:, 1:]\n",
    "X_train = np.zeros([N, D], dtype = float)\n",
    "Y_train = np.zeros(N, dtype = int)\n",
    "X_test = np.zeros([N_test, D], dtype = float)\n",
    "Y_test = np.zeros(N_test, dtype = int)\n",
    "j = 0\n",
    "k = 0\n",
    "for i in range(len(df_train)):\n",
    "    if i in train_indices:\n",
    "        X_train[j, :] = df_train.iloc[i, 1:-1]\n",
    "        Y_train[j] = df_train.iloc[i, -1]\n",
    "        j += 1\n",
    "    else:\n",
    "        X_test[k, :] = df_train.iloc[i, 1:-1]\n",
    "        Y_test[k] = df_train.iloc[i, -1]\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will apply a basic normalization to each column. I am careful to normalize everything using only the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(D):\n",
    "    mu = np.mean(X_train[:, i])\n",
    "    stddev = np.std(X_train[:, i])\n",
    "    X_train[:, i] = (X_train[:, i] - mu ) / stddev\n",
    "    X_test[:, i] = (X_test[:, i] - mu ) / stddev\n",
    "    X_predict[:, i] = (X_predict[:, i] - mu ) / stddev\n",
    "\n",
    "# We require Y_train and Y_test to be from 0-3, not 1-4\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I actually need to onehot encode the labels to the data set. In effect, my neural net will have 4 output units and I want the labels to emulate this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = len(np.unique(Y_train))\n",
    "Y_train_oh = np.zeros([len(Y_train), C])\n",
    "Y_test_oh = np.zeros([len(Y_test), C])\n",
    "for i in range(len(Y_train)):\n",
    "    y = Y_train[i] - 1\n",
    "    Y_train_oh[i, y] = 1\n",
    "for i in range(len(Y_test)):\n",
    "    y = Y_test[i] - 1\n",
    "    Y_test_oh[i, y] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to write a dataset class in order to use this set with pytorch. This is totally barebones, but I don't need to worry about streaming the dataset off the hard drive to multiple cores, since I have the memory to just store the entire dataset on each core. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"Dataset object for pytorch.\"\n",
    "    def __init__(self, X, Y):\n",
    "        'Initialization'\n",
    "        self.Y = Y.astype(float)\n",
    "        self.X = X.astype(float)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        x = self.X[index]\n",
    "        y = self.Y[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this class to actually construct dataset objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train, Y_train_oh)\n",
    "test_dataset = Dataset(X_test, Y_test_oh)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use my GPU to try and speed up the neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "# When you are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll take a first stab at the model architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=9, out_features=100, bias=True)\n",
      "  (1): Linear(in_features=100, out_features=500, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.2, inplace=False)\n",
      "  (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Dropout(p=0.2, inplace=False)\n",
      "  (7): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Dropout(p=0.2, inplace=False)\n",
      "  (10): Linear(in_features=100, out_features=4, bias=True)\n",
      "  (11): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(D, 100),\n",
    "    nn.Linear(100, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(500, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(500, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(100, C),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(1e-4))\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will import some helper functions that I wrote in problem set 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(12):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Erase accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data.float())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(output, target.float())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight update\n",
    "            optimizer.step()\n",
    "\n",
    "        # Track loss each epoch\n",
    "        print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))\n",
    "\n",
    "def get_train_err():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    train_error = 0\n",
    "    train_loss = 0\n",
    "    # Turning off automatic differentiation\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data.float())\n",
    "            train_loss += loss_fn(output, target).item() * len(target) # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=False).cpu().numpy()  # Get the index of the max class score\n",
    "            \n",
    "            # Convert the target back from onehot encoding\n",
    "            target = target.cpu().numpy()\n",
    "            target = target[:, 1] + target[:, 2] * 2 + target[:, 3] * 3\n",
    "            \n",
    "            # Determine the accuracy of the classification\n",
    "            correct += np.sum(pred==target)\n",
    "            train_error += roc_auc_score(target, \n",
    "                                         output.cpu(), \n",
    "                                         multi_class='ovr') * len(target)\n",
    "            \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_error /= len(train_loader.dataset)\n",
    "    print(\"Average Training ROC AUC: %.3f\"%train_error)\n",
    "    print('Training set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "          (train_loss, correct, len(train_loader.dataset),\n",
    "           100. * correct / len(train_loader.dataset)))\n",
    "    \n",
    "def get_test_err():\n",
    "    # Putting layers like Dropout into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_error = 0\n",
    "\n",
    "    # Turning off automatic differentiation\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data.float())\n",
    "            test_loss += loss_fn(output, target).item() * len(target)  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=False).cpu().numpy()  # Get the index of the max class score\n",
    "            \n",
    "            # Convert the target back from onehot encoding\n",
    "            target = target.cpu().numpy()\n",
    "            target = target[:, 1] + target[:, 2] * 2 + target[:, 3] * 3\n",
    "            \n",
    "            # Determine the accuracy of the classification\n",
    "            correct += np.sum(pred==target)\n",
    "            test_error += roc_auc_score(target, \n",
    "                                        output.cpu(), \n",
    "                                        multi_class='ovr') * len(target)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_error /= len(test_loader.dataset)\n",
    "    print(\"Average Testing ROC AUC: %.3f\"%test_error)\n",
    "    print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "          (test_loss, correct, len(test_loader.dataset),\n",
    "           100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.0661\n",
      "Train Epoch: 2  Loss: 0.0675\n",
      "Train Epoch: 3  Loss: 0.0686\n",
      "Train Epoch: 4  Loss: 0.0638\n",
      "Train Epoch: 5  Loss: 0.0663\n",
      "Train Epoch: 6  Loss: 0.0695\n",
      "Train Epoch: 7  Loss: 0.0639\n",
      "Train Epoch: 8  Loss: 0.0685\n",
      "Train Epoch: 9  Loss: 0.0647\n",
      "Train Epoch: 10  Loss: 0.0616\n",
      "Train Epoch: 11  Loss: 0.0625\n",
      "Train Epoch: 12  Loss: 0.0636\n",
      "Average Training ROC AUC: 0.762\n",
      "Training set: Average loss: 0.0634, Accuracy: 125764/200395 (62.7581)\n",
      "Average Testing ROC AUC: 0.761\n",
      "Test set: Average loss: 0.0637, Accuracy: 41794/66799 (62.5668)\n"
     ]
    }
   ],
   "source": [
    "train_model()\n",
    "get_train_err()\n",
    "get_test_err()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now try constructing and testing an **even deeper** neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (start): Linear(in_features=9, out_features=100, bias=True)\n",
      "  (expand1): Linear(in_features=100, out_features=5000, bias=True)\n",
      "  (expand2): ReLU()\n",
      "  (expand3): Dropout(p=0.05, inplace=False)\n",
      "  (1-0): Linear(in_features=5000, out_features=5000, bias=True)\n",
      "  (2-0): ReLU()\n",
      "  (3-0): Dropout(p=0.05, inplace=False)\n",
      "  (1-1): Linear(in_features=5000, out_features=5000, bias=True)\n",
      "  (2-1): ReLU()\n",
      "  (3-1): Dropout(p=0.05, inplace=False)\n",
      "  (1-2): Linear(in_features=5000, out_features=5000, bias=True)\n",
      "  (2-2): ReLU()\n",
      "  (3-2): Dropout(p=0.05, inplace=False)\n",
      "  (1-3): Linear(in_features=5000, out_features=5000, bias=True)\n",
      "  (2-3): ReLU()\n",
      "  (3-3): Dropout(p=0.05, inplace=False)\n",
      "  (narrow1): Linear(in_features=5000, out_features=100, bias=True)\n",
      "  (narrow2): ReLU()\n",
      "  (narrow3): Dropout(p=0.05, inplace=False)\n",
      "  (final-1): Linear(in_features=100, out_features=4, bias=True)\n",
      "  (final-2): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "height = 5000\n",
    "narrow = 100\n",
    "drop = 0.05\n",
    "depth = 4\n",
    "ordict = OrderedDict()\n",
    "ordict['start'] = nn.Linear(D, narrow)\n",
    "ordict['expand1'] = nn.Linear(narrow, height)\n",
    "ordict['expand2'] = nn.ReLU()\n",
    "ordict['expand3'] = nn.Dropout(drop)\n",
    "\n",
    "# Construct the bulk of the net\n",
    "for i in range(depth):\n",
    "    ordict['1-%i'%i] = nn.Linear(height, height)\n",
    "    ordict['2-%i'%i] = nn.ReLU()\n",
    "    ordict['3-%i'%i] = nn.Dropout(drop)\n",
    "    \n",
    "# Narrow the net and bring it down to the last few nodes\n",
    "ordict['narrow1'] = nn.Linear(height, narrow)\n",
    "ordict['narrow2'] = nn.ReLU()\n",
    "ordict['narrow3'] = nn.Dropout(drop)\n",
    "ordict['final-1'] = nn.Linear(narrow, C)\n",
    "ordict['final-2'] = nn.Softmax(dim=1)\n",
    "\n",
    "# Pack all the layers into the model\n",
    "model = nn.Sequential(ordict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(1e-4))\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.0625\n",
      "Train Epoch: 2  Loss: 0.0669\n",
      "Train Epoch: 3  Loss: 0.0651\n",
      "Train Epoch: 4  Loss: 0.0655\n",
      "Train Epoch: 5  Loss: 0.0647\n",
      "Train Epoch: 6  Loss: 0.0632\n",
      "Train Epoch: 7  Loss: 0.0616\n",
      "Train Epoch: 8  Loss: 0.0644\n",
      "Train Epoch: 9  Loss: 0.0597\n",
      "Train Epoch: 10  Loss: 0.0634\n",
      "Train Epoch: 11  Loss: 0.0572\n",
      "Train Epoch: 12  Loss: 0.0630\n",
      "Average Training ROC AUC: 0.785\n",
      "Training set: Average loss: 0.0609, Accuracy: 128575/200395 (64.1608)\n",
      "Average Testing ROC AUC: 0.781\n",
      "Test set: Average loss: 0.0618, Accuracy: 42502/66799 (63.6267)\n"
     ]
    }
   ],
   "source": [
    "train_model()\n",
    "get_train_err()\n",
    "get_test_err()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model looks good. Let's train it on everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.0603\n",
      "Train Epoch: 2  Loss: 0.0621\n",
      "Train Epoch: 3  Loss: 0.0582\n",
      "Train Epoch: 4  Loss: 0.0593\n",
      "Train Epoch: 5  Loss: 0.0588\n",
      "Train Epoch: 6  Loss: 0.0654\n",
      "Train Epoch: 7  Loss: 0.0609\n",
      "Train Epoch: 8  Loss: 0.0623\n",
      "Train Epoch: 9  Loss: 0.0614\n",
      "Train Epoch: 10  Loss: 0.0617\n",
      "Train Epoch: 11  Loss: 0.0597\n",
      "Train Epoch: 12  Loss: 0.0552\n",
      "Average Training ROC AUC: 0.803\n",
      "Training set: Average loss: 0.0590, Accuracy: 175294/267194 (65.6055)\n"
     ]
    }
   ],
   "source": [
    "Y_oh = np.zeros([N_total, C])\n",
    "X = np.zeros([N_total, D])\n",
    "Y_oh[:N, :] = Y_train_oh\n",
    "Y_oh[N:, :] = Y_test_oh\n",
    "X[:N, :] = X_train\n",
    "X[N:, :] = X_test\n",
    "train_dataset = Dataset(X, Y_oh)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# Redefine the model\n",
    "model = nn.Sequential(ordict)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(1e-4))\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "# Retrain the model on all avaliable data and get the training error. \n",
    "train_model()\n",
    "get_train_err()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is well trained, I will predict the labels and submit to kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = np.zeros([len(X_predict), 5])\n",
    "output = 0\n",
    "with torch.no_grad():\n",
    "    pred_data[:, 1:] = model(torch.tensor(X_predict).to(device).float()).cpu()\n",
    "pred_data[:, 0] = df_test[\"id\"].values\n",
    "pred_df = pd.DataFrame(pred_data, columns = [\"id\", \"P1\", \"P2\", \"P3\", \"P4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df.astype({'id': 'int'})\n",
    "pred_df.to_csv(\"./FakeDeep_Attempt2_NeuralNet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNEcboXAHDpYYd97FWOW++h",
   "name": "Untitled"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
