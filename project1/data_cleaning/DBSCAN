from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

def dbscan_cleanse(X, e=0.5, min=5):
  # Performs DBSCAN clustering with:
  # eps: max distance between samples to be consider neighbors
  # min_samples: min number samples in neighborhood for point to be considered a core point.
  # Returns new dataframe. MUST ENCODE PRIOR.
  db = DBSCAN(eps=e, min_samples=min).fit(X.values)

  # Cluster labels for each point in dataset. Noisy points labeled -1.
  labels = db.labels_

  # number of clusters and noisy points
  num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
  num_noisy = list(labels).count(-1)
  print('Estimated number of clusters: %d' % num_clusters)
  print('Estimated number of noise points: %d' % num_noisy)

  # remove the outliers
  labels = labels.reshape((len(labels), 1))
  index_drop = []
  i = 0
  for i in range(len(labels)):
    if labels[i] == -1: index_drop.append(i)
    i+=1
  new_df = X.drop(X.index[index_drop])
  return new_df
