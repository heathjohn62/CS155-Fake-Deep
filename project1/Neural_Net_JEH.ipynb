{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import OrderedDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed random number generation\n",
    "torch.manual_seed(62)\n",
    "np.random.seed(62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Model\n",
    "### Fake Deep - CMS 155\n",
    "I will first import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIPS_NAME</th>\n",
       "      <th>FIPS_CODE</th>\n",
       "      <th>SOURCE_REPORTING_UNIT_NAME</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>38.205000</td>\n",
       "      <td>-120.335000</td>\n",
       "      <td>0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33.813100</td>\n",
       "      <td>-85.104300</td>\n",
       "      <td>1</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>82</td>\n",
       "      <td>143.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>32.201000</td>\n",
       "      <td>-82.498700</td>\n",
       "      <td>1</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>130</td>\n",
       "      <td>209.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32.509300</td>\n",
       "      <td>-81.708600</td>\n",
       "      <td>1</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>19</td>\n",
       "      <td>31.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>33.663889</td>\n",
       "      <td>-116.171944</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   LATITUDE   LONGITUDE  STATE  DISCOVERY_TIME  FIRE_SIZE  FIPS_NAME  \\\n",
       "0   0  38.205000 -120.335000      0           130.0       0.10        215   \n",
       "1   1  33.813100  -85.104300      1          1115.0       1.17         82   \n",
       "2   2  32.201000  -82.498700      1          1600.0       0.07        130   \n",
       "3   3  32.509300  -81.708600      1          1215.0       4.40         19   \n",
       "4   4  33.663889 -116.171944      0             0.0       0.20        215   \n",
       "\n",
       "   FIPS_CODE  SOURCE_REPORTING_UNIT_NAME  DATE  LABEL  \n",
       "0        0.0                         157     0      1  \n",
       "1      143.0                          71     0      4  \n",
       "2      209.0                          71     0      2  \n",
       "3       31.0                          71     0      4  \n",
       "4        0.0                          14     0      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./datasets/jeh_train_label-enc_iter-imp_linear-norm.csv\")\n",
    "# Rather than do real imputation,I just replace nans with zeros\n",
    "df_train = df_train.fillna(0)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIPS_NAME</th>\n",
       "      <th>FIPS_CODE</th>\n",
       "      <th>SOURCE_REPORTING_UNIT_NAME</th>\n",
       "      <th>DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>285382</td>\n",
       "      <td>34.346944</td>\n",
       "      <td>-117.442222</td>\n",
       "      <td>0</td>\n",
       "      <td>1605.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>158</td>\n",
       "      <td>71.0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285383</td>\n",
       "      <td>34.020390</td>\n",
       "      <td>-116.179970</td>\n",
       "      <td>0</td>\n",
       "      <td>1545.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285384</td>\n",
       "      <td>38.068611</td>\n",
       "      <td>-120.276667</td>\n",
       "      <td>0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>196</td>\n",
       "      <td>109.0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285385</td>\n",
       "      <td>32.499971</td>\n",
       "      <td>-83.742573</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>87</td>\n",
       "      <td>153.0</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>285386</td>\n",
       "      <td>32.924940</td>\n",
       "      <td>-114.992530</td>\n",
       "      <td>0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>89</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   LATITUDE   LONGITUDE  STATE  DISCOVERY_TIME  FIRE_SIZE  FIPS_NAME  \\\n",
       "0  285382  34.346944 -117.442222      0          1605.0        0.2        158   \n",
       "1  285383  34.020390 -116.179970      0          1545.0        0.1        218   \n",
       "2  285384  38.068611 -120.276667      0          1200.0        0.1        196   \n",
       "3  285385  32.499971  -83.742573      1             0.0        0.4         87   \n",
       "4  285386  32.924940 -114.992530      0           126.0        0.1         89   \n",
       "\n",
       "   FIPS_CODE  SOURCE_REPORTING_UNIT_NAME  DATE  \n",
       "0       71.0                         145     0  \n",
       "1        0.0                          69     0  \n",
       "2      109.0                         170     0  \n",
       "3      153.0                          47     1  \n",
       "4       25.0                          18     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"./datasets/jeh_test_label-enc_iter-imp_linear-norm.csv\")\n",
    "df_test = df_test.fillna(0)\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I separate the data randomly into training and testing sets, with a 75/25 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(df_train.columns[1:-1])\n",
    "N_total = len(df_train)\n",
    "N = int(0.75 * N_total)\n",
    "N_test = N_total - N\n",
    "X_predict = df_test.to_numpy(dtype = float)[:, 1:]\n",
    "try:\n",
    "    X_train = np.load(\"./jeh_checkpoints/x_train.npy\")\n",
    "    Y_train = np.load(\"./jeh_checkpoints/y_train.npy\")\n",
    "    X_test = np.load(\"./jeh_checkpoints/x_test.npy\")\n",
    "    Y_test = np.load(\"./jeh_checkpoints/y_test.npy\")\n",
    "except FileNotFoundError:\n",
    "    test_indices = np.random.choice(list(range(N_total)), size=N_test, replace=False)\n",
    "    X_train = np.zeros([N, D], dtype = float)\n",
    "    Y_train = np.zeros(N, dtype = int)\n",
    "    X_test = np.zeros([N_test, D], dtype = float)\n",
    "    Y_test = np.zeros(N_test, dtype = int)\n",
    "    j = 0\n",
    "    k = 0\n",
    "    for i in range(len(df_train)):\n",
    "        if i not in test_indices:\n",
    "            X_train[j, :] = df_train.iloc[i, 1:-1]\n",
    "            Y_train[j] = df_train.iloc[i, -1]\n",
    "            j += 1\n",
    "        else:\n",
    "            X_test[k, :] = df_train.iloc[i, 1:-1]\n",
    "            Y_test[k] = df_train.iloc[i, -1]\n",
    "            k += 1\n",
    "    np.save(\"./jeh_checkpoints/x_train.npy\", X_train)\n",
    "    np.save(\"./jeh_checkpoints/y_train.npy\", Y_train)\n",
    "    np.save(\"./jeh_checkpoints/x_test.npy\", X_test)\n",
    "    np.save(\"./jeh_checkpoints/y_test.npy\", Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will apply a basic normalization to each column. I am careful to normalize everything using only the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(D):\n",
    "    mu = np.mean(X_train[:, i])\n",
    "    stddev = np.std(X_train[:, i])\n",
    "    X_train[:, i] = (X_train[:, i] - mu ) / stddev\n",
    "    X_test[:, i] = (X_test[:, i] - mu ) / stddev\n",
    "    X_predict[:, i] = (X_predict[:, i] - mu ) / stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We require Y_train and Y_test to be from 0-3, not 1-4\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I actually need to onehot encode the labels to the data set. In effect, my neural net will have 4 output units and I want the labels to emulate this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = len(np.unique(Y_train))\n",
    "Y_train_oh = np.zeros([len(Y_train), C])\n",
    "Y_test_oh = np.zeros([len(Y_test), C])\n",
    "for i in range(len(Y_train)):\n",
    "    y = Y_train[i] - 1\n",
    "    Y_train_oh[i, y] = 1\n",
    "for i in range(len(Y_test)):\n",
    "    y = Y_test[i] - 1\n",
    "    Y_test_oh[i, y] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to write a dataset class in order to use this set with pytorch. This is totally barebones, but I don't need to worry about streaming the dataset off the hard drive to multiple cores, since I have the memory to just store the entire dataset on each core. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"Dataset object for pytorch.\"\n",
    "    def __init__(self, X, Y):\n",
    "        'Initialization'\n",
    "        self.Y = Y.astype(float)\n",
    "        self.X = X.astype(float)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        x = self.X[index]\n",
    "        y = self.Y[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this class to actually construct dataset objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train, Y_train_oh)\n",
    "test_dataset = Dataset(X_test, Y_test_oh)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use my GPU to try and speed up the neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "# When you are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll take a first stab at the model architecture. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(D, 100),\n",
    "    nn.Linear(100, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(500, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(500, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(100, C),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(1e-4))\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will import some helper functions that I wrote in problem set 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(12):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Erase accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data.float())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(output, target.float())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight update\n",
    "            optimizer.step()\n",
    "\n",
    "        # Track loss each epoch\n",
    "        print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))\n",
    "\n",
    "def get_train_err():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    train_error = 0\n",
    "    train_loss = 0\n",
    "    # Turning off automatic differentiation\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data.float())\n",
    "            train_loss += loss_fn(output, target).item() * len(target) # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=False).cpu().numpy()  # Get the index of the max class score\n",
    "            \n",
    "            # Convert the target back from onehot encoding\n",
    "            target = target.cpu().numpy()\n",
    "            target = target[:, 1] + target[:, 2] * 2 + target[:, 3] * 3\n",
    "            print(target)\n",
    "            # Determine the accuracy of the classification\n",
    "            correct += np.sum(pred==target)\n",
    "            train_error += roc_auc_score(target, \n",
    "                                         output.cpu(), \n",
    "                                         multi_class='ovr') * len(target)\n",
    "            \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_error /= len(train_loader.dataset)\n",
    "    print(\"Average Training ROC AUC: %.3f\"%train_error)\n",
    "    print('Training set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "          (train_loss, correct, len(train_loader.dataset),\n",
    "           100. * correct / len(train_loader.dataset)))\n",
    "    \n",
    "def get_test_err():\n",
    "    # Putting layers like Dropout into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_error = 0\n",
    "\n",
    "    # Turning off automatic differentiation\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data.float())\n",
    "            test_loss += loss_fn(output, target).item() * len(target)  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=False).cpu().numpy()  # Get the index of the max class score\n",
    "            \n",
    "            # Convert the target back from onehot encoding\n",
    "            target = target.cpu().numpy()\n",
    "            target = target[:, 1] + target[:, 2] * 2 + target[:, 3] * 3\n",
    "            \n",
    "            # Determine the accuracy of the classification\n",
    "            correct += np.sum(pred==target)\n",
    "            test_error += roc_auc_score(target, \n",
    "                                        output.cpu(), \n",
    "                                        multi_class='ovr') * len(target)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_error /= len(test_loader.dataset)\n",
    "    print(\"Average Testing ROC AUC: %.3f\"%test_error)\n",
    "    print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "          (test_loss, correct, len(test_loader.dataset),\n",
    "           100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "train_model()\n",
    "get_train_err()\n",
    "get_test_err()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now try constructing and testing an **even deeper** neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (start): Linear(in_features=9, out_features=100, bias=True)\n",
      "  (expand1): Linear(in_features=100, out_features=5000, bias=True)\n",
      "  (expand2): ReLU()\n",
      "  (expand3): Dropout(p=0.05, inplace=False)\n",
      "  (1-0): Linear(in_features=5000, out_features=5000, bias=True)\n",
      "  (2-0): ReLU()\n",
      "  (3-0): Dropout(p=0.05, inplace=False)\n",
      "  (1-1): Linear(in_features=5000, out_features=5000, bias=True)\n",
      "  (2-1): ReLU()\n",
      "  (3-1): Dropout(p=0.05, inplace=False)\n",
      "  (1-2): Linear(in_features=5000, out_features=5000, bias=True)\n",
      "  (2-2): ReLU()\n",
      "  (3-2): Dropout(p=0.05, inplace=False)\n",
      "  (1-3): Linear(in_features=5000, out_features=5000, bias=True)\n",
      "  (2-3): ReLU()\n",
      "  (3-3): Dropout(p=0.05, inplace=False)\n",
      "  (narrow1): Linear(in_features=5000, out_features=100, bias=True)\n",
      "  (narrow2): ReLU()\n",
      "  (narrow3): Dropout(p=0.05, inplace=False)\n",
      "  (final-1): Linear(in_features=100, out_features=4, bias=True)\n",
      "  (final-2): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "height = 5000\n",
    "narrow = 100\n",
    "drop = 0.05\n",
    "depth = 4\n",
    "ordict = OrderedDict()\n",
    "ordict['start'] = nn.Linear(D, narrow)\n",
    "ordict['expand1'] = nn.Linear(narrow, height)\n",
    "ordict['expand2'] = nn.ReLU()\n",
    "ordict['expand3'] = nn.Dropout(drop)\n",
    "\n",
    "# Construct the bulk of the net\n",
    "for i in range(depth):\n",
    "    ordict['1-%i'%i] = nn.Linear(height, height)\n",
    "    ordict['2-%i'%i] = nn.ReLU()\n",
    "    ordict['3-%i'%i] = nn.Dropout(drop)\n",
    "    \n",
    "# Narrow the net and bring it down to the last few nodes\n",
    "ordict['narrow1'] = nn.Linear(height, narrow)\n",
    "ordict['narrow2'] = nn.ReLU()\n",
    "ordict['narrow3'] = nn.Dropout(drop)\n",
    "ordict['final-1'] = nn.Linear(narrow, C)\n",
    "ordict['final-2'] = nn.Softmax(dim=1)\n",
    "\n",
    "# Pack all the layers into the model\n",
    "model = nn.Sequential(ordict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(1e-4))\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will train the model and record the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 0. 3. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 2. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 3.]\n",
      "[3. 0. 0. ... 2. 0. 1.]\n",
      "[2. 3. 0. ... 0. 1. 2.]\n",
      "[0. 0. 0. ... 0. 0. 2.]\n",
      "[0. 0. 1. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 3. 0. 0.]\n",
      "[0. 2. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 3. 1. 0.]\n",
      "[0. 2. 0. ... 0. 0. 1.]\n",
      "[0. 0. 3. ... 2. 0. 2.]\n",
      "[0. 2. 2. ... 1. 2. 0.]\n",
      "[0. 0. 0. ... 0. 0. 2.]\n",
      "[0. 3. 2. ... 0. 1. 2.]\n",
      "[0. 0. 0. ... 0. 0. 3.]\n",
      "[2. 0. 0. ... 3. 0. 1.]\n",
      "[0. 1. 0. ... 2. 0. 0.]\n",
      "[1. 3. 1. ... 2. 0. 0.]\n",
      "[0. 2. 2. ... 0. 0. 2.]\n",
      "[0. 2. 1. ... 2. 1. 0.]\n",
      "[0. 0. 2. ... 0. 0. 2.]\n",
      "[1. 0. 3. ... 1. 0. 0.]\n",
      "[0. 0. 1. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 1. ... 0. 1. 2.]\n",
      "[0. 3. 2. ... 0. 1. 2.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 1. 0. ... 3. 0. 3.]\n",
      "[0. 0. 3. ... 0. 0. 0.]\n",
      "[2. 0. 3. ... 0. 2. 3.]\n",
      "[1. 2. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 2. 0.]\n",
      "[0. 0. 0. ... 2. 0. 0.]\n",
      "[2. 0. 0. ... 0. 2. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 1. ... 2. 1. 0.]\n",
      "[0. 0. 3. ... 0. 2. 2.]\n",
      "[2. 2. 2. ... 1. 0. 0.]\n",
      "[0. 2. 2. ... 0. 3. 1.]\n",
      "[0. 3. 0. ... 0. 1. 0.]\n",
      "[0. 0. 3. ... 3. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 2. ... 3. 2. 1.]\n",
      "[0. 0. 2. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 2. 0. 1.]\n",
      "[0. 1. 0. ... 3. 0. 2.]\n",
      "[0. 3. 0. ... 1. 0. 0.]\n",
      "[0. 2. 0. ... 0. 0. 3.]\n",
      "[0. 2. 0. ... 0. 3. 2.]\n",
      "[1. 3. 0. ... 1. 0. 0.]\n",
      "[1. 1. 0. ... 1. 1. 0.]\n",
      "[2. 0. 0. ... 0. 0. 1.]\n",
      "[2. 0. 0. ... 0. 0. 3.]\n",
      "[2. 0. 2. ... 0. 0. 1.]\n",
      "[2. 0. 2. ... 0. 0. 0.]\n",
      "[1. 2. 1. ... 0. 3. 0.]\n",
      "[0. 0. 2. ... 0. 0. 0.]\n",
      "[2. 2. 0. ... 0. 2. 0.]\n",
      "[0. 0. 1. ... 2. 2. 0.]\n",
      "[3. 0. 3. ... 1. 2. 1.]\n",
      "[2. 2. 0. ... 0. 3. 3.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 2. 0. 0.]\n",
      "[2. 1. 3. ... 2. 2. 0.]\n",
      "[0. 0. 3. ... 0. 0. 0.]\n",
      "[0. 0. 3. ... 0. 0. 0.]\n",
      "[0. 0. 2. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 1.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[1. 0. 3. ... 0. 0. 1.]\n",
      "[0. 2. 3. ... 0. 0. 0.]\n",
      "[2. 0. 1. ... 1. 0. 2.]\n",
      "[0. 1. 1. ... 3. 0. 0.]\n",
      "[0. 2. 0. ... 0. 2. 2.]\n",
      "[0. 2. 0. ... 0. 0. 0.]\n",
      "[0. 0. 2. ... 0. 0. 3.]\n",
      "[0. 2. 0. ... 0. 2. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 2. ... 2. 2. 1.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[3. 2. 0. ... 0. 0. 1.]\n",
      "[0. 0. 2. ... 0. 2. 2.]\n",
      "[2. 2. 2. ... 3. 2. 0.]\n",
      "[1. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 2. 1. 3.]\n",
      "[0. 0. 2. ... 0. 1. 1.]\n",
      "[3. 0. 2. ... 1. 2. 2.]\n",
      "[0. 0. 2. ... 0. 0. 0.]\n",
      "[0. 0. 3. ... 2. 0. 0.]\n",
      "[3. 0. 0. ... 0. 0. 2.]\n",
      "[2. 2. 1. ... 0. 2. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 2. 0. ... 0. 1. 1.]\n",
      "[0. 0. 0. ... 2. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 2. ... 0. 1. 0.]\n",
      "[0. 1. 0. ... 2. 0. 1.]\n",
      "[0. 0. 3. ... 1. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 1. 0. 2.]\n",
      "[1. 0. 1. ... 0. 2. 0.]\n",
      "[3. 0. 0. ... 1. 3. 1.]\n",
      "[3. 0. 3. ... 0. 0. 0.]\n",
      "[2. 0. 2. ... 0. 2. 1.]\n",
      "[1. 0. 0. ... 0. 2. 2.]\n",
      "[2. 3. 0. ... 0. 2. 0.]\n",
      "[0. 0. 1. ... 2. 2. 1.]\n",
      "[0. 0. 3. ... 2. 0. 1.]\n",
      "[3. 0. 3. ... 2. 0. 0.]\n",
      "[0. 0. 0. ... 2. 0. 0.]\n",
      "[0. 0. 3. ... 2. 0. 0.]\n",
      "[0. 2. 1. ... 0. 2. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 2. 3. 0.]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 3. 0. 1.]\n",
      "[3. 0. 0. ... 0. 0. 2.]\n",
      "[0. 2. 0. ... 0. 0. 2.]\n",
      "[0. 3. 0. ... 1. 0. 2.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[2. 0. 0. ... 1. 0. 0.]\n",
      "[0. 3. 1. ... 0. 0. 3.]\n",
      "[2. 3. 0. ... 1. 2. 0.]\n",
      "[2. 0. 0. ... 0. 1. 2.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 3. ... 2. 1. 0.]\n",
      "[1. 0. 0. ... 3. 2. 0.]\n",
      "[0. 2. 0. ... 0. 2. 0.]\n",
      "[3. 0. 0. ... 0. 0. 2.]\n",
      "[1. 0. 3. ... 2. 0. 2.]\n",
      "[0. 3. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 2. 2. 2.]\n",
      "[3. 1. 2. ... 3. 1. 0.]\n",
      "[0. 0. 0. ... 2. 0. 3.]\n",
      "[0. 2. 0. ... 1. 2. 1.]\n",
      "[1. 0. 3. ... 0. 2. 1.]\n",
      "[0. 0. 0. ... 2. 1. 0.]\n",
      "[0. 0. 0. ... 1. 2. 1.]\n",
      "[0. 1. 2. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 2.]\n",
      "[0. 0. 1. ... 1. 1. 0.]\n",
      "[0. 0. 0. ... 1. 1. 2.]\n",
      "[0. 0. 2. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 3.]\n",
      "[3. 1. 2. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 2. 3. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 2. ... 0. 1. 0.]\n",
      "[0. 0. 1. ... 0. 3. 1.]\n",
      "[3. 0. 0. ... 3. 0. 2.]\n",
      "[1. 0. 3. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 1. 0.]\n",
      "[0. 0. 2. ... 0. 2. 3.]\n",
      "[0. 0. 2. ... 2. 0. 2.]\n",
      "[0. 1. 1. ... 0. 3. 0.]\n",
      "[3. 0. 3. ... 2. 0. 0.]\n",
      "[0. 2. 1. ... 0. 0. 0.]\n",
      "[2. 1. 2. ... 0. 0. 2.]\n",
      "[1. 2. 0. ... 1. 1. 0.]\n",
      "[0. 0. 1. ... 1. 3. 1.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[2. 0. 1. ... 2. 0. 1.]\n",
      "[0. 2. 2. ... 3. 2. 0.]\n",
      "[0. 0. 2. ... 0. 1. 0.]\n",
      "[0. 3. 2. ... 1. 3. 1.]\n",
      "[0. 1. 0. ... 0. 1. 2.]\n",
      "[0. 0. 0. ... 0. 3. 0.]\n",
      "[3. 0. 3. ... 2. 1. 0.]\n",
      "[0. 2. 2. ... 1. 0. 3.]\n",
      "[3. 0. 3. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 3. ... 3. 3. 0.]\n",
      "[0. 2. 3. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[3. 3. 0. ... 0. 0. 2.]\n",
      "[0. 2. 1. ... 0. 0. 1.]\n",
      "[1. 2. 2. ... 1. 0. 1.]\n",
      "[0. 2. 0. ... 3. 0. 3.]\n",
      "[0. 1. 2. ... 0. 0. 2.]\n",
      "[0. 1. 0. ... 1. 0. 1.]\n",
      "[3. 3. 1. ... 0. 1. 0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[3. 3. 3. ... 1. 0. 1.]\n",
      "[0. 3. 0. ... 3. 0. 2.]\n",
      "[0. 0. 0. ... 3. 3. 0.]\n",
      "[0. 0. 0. ... 0. 3. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 3. 1. ... 0. 0. 2.]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 2. 0. 0.]\n",
      "[0. 0. 0. ... 0. 2. 2.]\n",
      "[3. 1. 2. ... 0. 3. 0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[2. 0. 0. ... 2. 0. 1.]\n",
      "[1. 0. 0. ... 0. 1. 1.]\n",
      "[0. 1. 0. ... 2. 0. 0.]\n",
      "[0. 1. 0. ... 2. 3. 2.]\n",
      "[0. 0. 0. ... 3. 0. 2.]\n",
      "[3. 2. 2. ... 3. 0. 1.]\n",
      "[0. 0. 2. ... 0. 1. 0.]\n",
      "[1. 0. 0. 0. 1. 2. 1. 2. 3. 0. 3. 3. 0. 0. 0. 1. 3. 1. 0. 0.]\n",
      "Average Training ROC AUC: 0.781\n",
      "Training set: Average loss: 0.0621, Accuracy: 135855/214036 (63.4730)\n",
      "Average Testing ROC AUC: 0.780\n",
      "Test set: Average loss: 0.0624, Accuracy: 45112/71346 (63.2299)\n"
     ]
    }
   ],
   "source": [
    "# train_model()\n",
    "get_train_err()\n",
    "get_test_err()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the testing error like I will have to later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41059932920716025"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "pred_data = np.zeros([len(X_test), 4])\n",
    "with torch.no_grad():\n",
    "    pred_data[:, :] = model(torch.tensor(X_test).to(device).float()).cpu()\n",
    "\n",
    "roc_auc_score(Y_test, pred_data, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71346"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model looks good. Let's train it on everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.0622\n",
      "Train Epoch: 2  Loss: 0.0656\n",
      "Train Epoch: 3  Loss: 0.0586\n",
      "Train Epoch: 4  Loss: 0.0631\n",
      "Train Epoch: 5  Loss: 0.0630\n",
      "Train Epoch: 6  Loss: 0.0575\n",
      "Train Epoch: 7  Loss: 0.0648\n",
      "Train Epoch: 8  Loss: 0.0609\n",
      "Train Epoch: 9  Loss: 0.0628\n",
      "Train Epoch: 10  Loss: 0.0578\n",
      "Train Epoch: 11  Loss: 0.0617\n",
      "Train Epoch: 12  Loss: 0.0624\n",
      "Average Training ROC AUC: 0.802\n",
      "Training set: Average loss: 0.0599, Accuracy: 185233/285382 (64.9070)\n"
     ]
    }
   ],
   "source": [
    "Y_oh = np.zeros([N_total, C])\n",
    "X = np.zeros([N_total, D])\n",
    "Y_oh[:N, :] = Y_train_oh\n",
    "Y_oh[N:, :] = Y_test_oh\n",
    "X[:N, :] = X_train\n",
    "X[N:, :] = X_test\n",
    "train_dataset = Dataset(X, Y_oh)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# Redefine the model\n",
    "model = nn.Sequential(ordict)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(1e-4))\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "# Retrain the model on all avaliable data and get the training error. \n",
    "train_model()\n",
    "get_train_err()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is well trained, I will predict the labels and submit to kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_data = np.zeros([len(X_predict), 5])\n",
    "with torch.no_grad():\n",
    "    pred_data[:, 1:] = model(torch.tensor(X_predict).to(device).float()).cpu()\n",
    "pred_data[:, 0] = df_test[\"id\"].values\n",
    "pred_df = pd.DataFrame(pred_data, columns = [\"id\", \"P1\", \"P2\", \"P3\", \"P4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df.astype({'id': 'int'})\n",
    "pred_df.to_csv(\"./FakeDeep_Attempt4_NeuralNet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>285382</td>\n",
       "      <td>0.508899</td>\n",
       "      <td>0.160748</td>\n",
       "      <td>0.289178</td>\n",
       "      <td>0.041175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285383</td>\n",
       "      <td>0.322410</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.094937</td>\n",
       "      <td>0.568750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285384</td>\n",
       "      <td>0.419784</td>\n",
       "      <td>0.233136</td>\n",
       "      <td>0.227026</td>\n",
       "      <td>0.120054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285385</td>\n",
       "      <td>0.287099</td>\n",
       "      <td>0.516503</td>\n",
       "      <td>0.118180</td>\n",
       "      <td>0.078218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>285386</td>\n",
       "      <td>0.377663</td>\n",
       "      <td>0.224787</td>\n",
       "      <td>0.331835</td>\n",
       "      <td>0.065715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73030</th>\n",
       "      <td>358412</td>\n",
       "      <td>0.861071</td>\n",
       "      <td>0.081721</td>\n",
       "      <td>0.041671</td>\n",
       "      <td>0.015536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73031</th>\n",
       "      <td>358413</td>\n",
       "      <td>0.335623</td>\n",
       "      <td>0.069149</td>\n",
       "      <td>0.587107</td>\n",
       "      <td>0.008120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73032</th>\n",
       "      <td>358414</td>\n",
       "      <td>0.604119</td>\n",
       "      <td>0.100185</td>\n",
       "      <td>0.287817</td>\n",
       "      <td>0.007879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73033</th>\n",
       "      <td>358415</td>\n",
       "      <td>0.573586</td>\n",
       "      <td>0.106984</td>\n",
       "      <td>0.297592</td>\n",
       "      <td>0.021839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73034</th>\n",
       "      <td>358416</td>\n",
       "      <td>0.364558</td>\n",
       "      <td>0.148368</td>\n",
       "      <td>0.343405</td>\n",
       "      <td>0.143669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73035 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        P1        P2        P3        P4\n",
       "0      285382  0.508899  0.160748  0.289178  0.041175\n",
       "1      285383  0.322410  0.013904  0.094937  0.568750\n",
       "2      285384  0.419784  0.233136  0.227026  0.120054\n",
       "3      285385  0.287099  0.516503  0.118180  0.078218\n",
       "4      285386  0.377663  0.224787  0.331835  0.065715\n",
       "...       ...       ...       ...       ...       ...\n",
       "73030  358412  0.861071  0.081721  0.041671  0.015536\n",
       "73031  358413  0.335623  0.069149  0.587107  0.008120\n",
       "73032  358414  0.604119  0.100185  0.287817  0.007879\n",
       "73033  358415  0.573586  0.106984  0.297592  0.021839\n",
       "73034  358416  0.364558  0.148368  0.343405  0.143669\n",
       "\n",
       "[73035 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./FakeDeep_Attempt4_NeuralNet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>285382</td>\n",
       "      <td>0.508899</td>\n",
       "      <td>0.160748</td>\n",
       "      <td>0.289178</td>\n",
       "      <td>0.041175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285383</td>\n",
       "      <td>0.322410</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.094937</td>\n",
       "      <td>0.568750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285384</td>\n",
       "      <td>0.419784</td>\n",
       "      <td>0.233136</td>\n",
       "      <td>0.227026</td>\n",
       "      <td>0.120054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285385</td>\n",
       "      <td>0.287099</td>\n",
       "      <td>0.516503</td>\n",
       "      <td>0.118180</td>\n",
       "      <td>0.078218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>285386</td>\n",
       "      <td>0.377663</td>\n",
       "      <td>0.224787</td>\n",
       "      <td>0.331835</td>\n",
       "      <td>0.065715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73030</th>\n",
       "      <td>358412</td>\n",
       "      <td>0.861071</td>\n",
       "      <td>0.081721</td>\n",
       "      <td>0.041671</td>\n",
       "      <td>0.015536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73031</th>\n",
       "      <td>358413</td>\n",
       "      <td>0.335623</td>\n",
       "      <td>0.069149</td>\n",
       "      <td>0.587107</td>\n",
       "      <td>0.008120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73032</th>\n",
       "      <td>358414</td>\n",
       "      <td>0.604119</td>\n",
       "      <td>0.100185</td>\n",
       "      <td>0.287817</td>\n",
       "      <td>0.007879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73033</th>\n",
       "      <td>358415</td>\n",
       "      <td>0.573586</td>\n",
       "      <td>0.106984</td>\n",
       "      <td>0.297592</td>\n",
       "      <td>0.021839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73034</th>\n",
       "      <td>358416</td>\n",
       "      <td>0.364558</td>\n",
       "      <td>0.148368</td>\n",
       "      <td>0.343405</td>\n",
       "      <td>0.143669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73035 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        P1        P2        P3        P4\n",
       "0      285382  0.508899  0.160748  0.289178  0.041175\n",
       "1      285383  0.322410  0.013904  0.094937  0.568750\n",
       "2      285384  0.419784  0.233136  0.227026  0.120054\n",
       "3      285385  0.287099  0.516503  0.118180  0.078218\n",
       "4      285386  0.377663  0.224787  0.331835  0.065715\n",
       "...       ...       ...       ...       ...       ...\n",
       "73030  358412  0.861071  0.081721  0.041671  0.015536\n",
       "73031  358413  0.335623  0.069149  0.587107  0.008120\n",
       "73032  358414  0.604119  0.100185  0.287817  0.007879\n",
       "73033  358415  0.573586  0.106984  0.297592  0.021839\n",
       "73034  358416  0.364558  0.148368  0.343405  0.143669\n",
       "\n",
       "[73035 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./FakeDeep_Attempt4_NeuralNet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNEcboXAHDpYYd97FWOW++h",
   "name": "Untitled"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
