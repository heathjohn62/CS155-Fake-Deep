{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Recurrent_neural_nets.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOlIZXsj3Qyjxd0uZh02SX8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"X-MHCGJ0PSYO","executionInfo":{"status":"ok","timestamp":1615350356558,"user_tz":600,"elapsed":1771,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}}},"source":["import pandas as pd\n","import numpy as np\n","import urllib\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils\n","np.random.seed(62)\n","tf.random.set_seed(62)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uid2_umX7sDk"},"source":["\n","When you run this notebook, make sure that you set the hardware accelerator to GPU. The nerual net trains much faster this way. "]},{"cell_type":"markdown","metadata":{"id":"Df2z2xJSTqti"},"source":["I'm roughly following a tutorial on using keras for developing recurrent nerual networks here: \n","\n","https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n","\n","First I will download shakespeare's sonnets. "]},{"cell_type":"code","metadata":{"id":"Da4IE5vrRRcP","executionInfo":{"status":"ok","timestamp":1615350356560,"user_tz":600,"elapsed":1766,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}}},"source":["!mkdir dataset"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWAhW3V_QdO3","executionInfo":{"status":"ok","timestamp":1615350356819,"user_tz":600,"elapsed":2018,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}},"outputId":"b9f7e633-66a0-4f18-8cdf-3dd35dc26010"},"source":["urllib.request.urlretrieve(\n","    'https://raw.githubusercontent.com/lakigigar/Caltech-CS155-2021/main/projects/project3/data/shakespeare.txt', \n","                           './dataset/shakespeare.txt')"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('./dataset/shakespeare.txt', <http.client.HTTPMessage at 0x7fab43c54990>)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"34tEnF8Ds2Z0"},"source":["Next I load the sonnets into memory, convert them to lowercase, strip out unnecessary whitespace and add a terminal * character to indicate the end of the sonnet. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3h5LBYQR4v2","executionInfo":{"status":"ok","timestamp":1615350356820,"user_tz":600,"elapsed":2013,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}},"outputId":"36569f9f-f269-4bc2-9934-9836bd75c0fc"},"source":["# load ascii text and covert to lowercase\n","filename = './dataset/shakespeare.txt'\n","raw_text = open(filename, 'r', encoding='utf-8').read()\n","raw_text = raw_text.lower()\n","sonnets = raw_text.split('\\n\\n')\n","sonnets[0] = '\\n' + sonnets[0]\n","N = len(sonnets)\n","for i in range(N):\n","  sonnet = sonnets[i][1:] # extract the sonnet, minus the newline at the beginning\n","  index = sonnet.index('\\n')\n","  sonnet = sonnet[index + 1:] + \"*\" # I am using an astrix to mark the end of a sonnet.\n","  sonnets[i] = sonnet\n","\n","sonnets[:10]"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"from fairest creatures we desire increase,\\nthat thereby beauty's rose might never die,\\nbut as the riper should by time decease,\\nhis tender heir might bear his memory:\\nbut thou contracted to thine own bright eyes,\\nfeed'st thy light's flame with self-substantial fuel,\\nmaking a famine where abundance lies,\\nthy self thy foe, to thy sweet self too cruel:\\nthou that art now the world's fresh ornament,\\nand only herald to the gaudy spring,\\nwithin thine own bud buriest thy content,\\nand tender churl mak'st waste in niggarding:\\n  pity the world, or else this glutton be,\\n  to eat the world's due, by the grave and thee.*\",\n"," \"when forty winters shall besiege thy brow,\\nand dig deep trenches in thy beauty's field,\\nthy youth's proud livery so gazed on now,\\nwill be a tattered weed of small worth held:\\nthen being asked, where all thy beauty lies,\\nwhere all the treasure of thy lusty days;\\nto say within thine own deep sunken eyes,\\nwere an all-eating shame, and thriftless praise.\\nhow much more praise deserved thy beauty's use,\\nif thou couldst answer 'this fair child of mine\\nshall sum my count, and make my old excuse'\\nproving his beauty by succession thine.\\n  this were to be new made when thou art old,\\n  and see thy blood warm when thou feel'st it cold.*\",\n"," \"look in thy glass and tell the face thou viewest,\\nnow is the time that face should form another,\\nwhose fresh repair if now thou not renewest,\\nthou dost beguile the world, unbless some mother.\\nfor where is she so fair whose uneared womb\\ndisdains the tillage of thy husbandry?\\nor who is he so fond will be the tomb,\\nof his self-love to stop posterity?\\nthou art thy mother's glass and she in thee\\ncalls back the lovely april of her prime,\\nso thou through windows of thine age shalt see,\\ndespite of wrinkles this thy golden time.\\n  but if thou live remembered not to be,\\n  die single and thine image dies with thee.*\",\n"," \"unthrifty loveliness why dost thou spend,\\nupon thy self thy beauty's legacy?\\nnature's bequest gives nothing but doth lend,\\nand being frank she lends to those are free:\\nthen beauteous niggard why dost thou abuse,\\nthe bounteous largess given thee to give?\\nprofitless usurer why dost thou use\\nso great a sum of sums yet canst not live?\\nfor having traffic with thy self alone,\\nthou of thy self thy sweet self dost deceive,\\nthen how when nature calls thee to be gone,\\nwhat acceptable audit canst thou leave?\\n  thy unused beauty must be tombed with thee,\\n  which used lives th' executor to be.*\",\n"," \"those hours that with gentle work did frame\\nthe lovely gaze where every eye doth dwell\\nwill play the tyrants to the very same,\\nand that unfair which fairly doth excel:\\nfor never-resting time leads summer on\\nto hideous winter and confounds him there,\\nsap checked with frost and lusty leaves quite gone,\\nbeauty o'er-snowed and bareness every where:\\nthen were not summer's distillation left\\na liquid prisoner pent in walls of glass,\\nbeauty's effect with beauty were bereft,\\nnor it nor no remembrance what it was.\\n  but flowers distilled though they with winter meet,\\n  leese but their show, their substance still lives sweet.*\",\n"," \"then let not winter's ragged hand deface,\\nin thee thy summer ere thou be distilled:\\nmake sweet some vial; treasure thou some place,\\nwith beauty's treasure ere it be self-killed:\\nthat use is not forbidden usury,\\nwhich happies those that pay the willing loan;\\nthat's for thy self to breed another thee,\\nor ten times happier be it ten for one,\\nten times thy self were happier than thou art,\\nif ten of thine ten times refigured thee:\\nthen what could death do if thou shouldst depart,\\nleaving thee living in posterity?\\n  be not self-willed for thou art much too fair,\\n  to be death's conquest and make worms thine heir.*\",\n"," 'lo in the orient when the gracious light\\nlifts up his burning head, each under eye\\ndoth homage to his new-appearing sight,\\nserving with looks his sacred majesty,\\nand having climbed the steep-up heavenly hill,\\nresembling strong youth in his middle age,\\nyet mortal looks adore his beauty still,\\nattending on his golden pilgrimage:\\nbut when from highmost pitch with weary car,\\nlike feeble age he reeleth from the day,\\nthe eyes (fore duteous) now converted are\\nfrom his low tract and look another way:\\n  so thou, thy self out-going in thy noon:\\n  unlooked on diest unless thou get a son.*',\n"," \"music to hear, why hear'st thou music sadly?\\nsweets with sweets war not, joy delights in joy:\\nwhy lov'st thou that which thou receiv'st not gladly,\\nor else receiv'st with pleasure thine annoy?\\nif the true concord of well-tuned sounds,\\nby unions married do offend thine ear,\\nthey do but sweetly chide thee, who confounds\\nin singleness the parts that thou shouldst bear:\\nmark how one string sweet husband to another,\\nstrikes each in each by mutual ordering;\\nresembling sire, and child, and happy mother,\\nwho all in one, one pleasing note do sing:\\n  whose speechless song being many, seeming one,\\n  sings this to thee, 'thou single wilt prove none'.*\",\n"," \"is it for fear to wet a widow's eye,\\nthat thou consum'st thy self in single life?\\nah, if thou issueless shalt hap to die,\\nthe world will wail thee like a makeless wife,\\nthe world will be thy widow and still weep,\\nthat thou no form of thee hast left behind,\\nwhen every private widow well may keep,\\nby children's eyes, her husband's shape in mind:\\nlook what an unthrift in the world doth spend\\nshifts but his place, for still the world enjoys it;\\nbut beauty's waste hath in the world an end,\\nand kept unused the user so destroys it:\\n  no love toward others in that bosom sits\\n  that on himself such murd'rous shame commits.*\",\n"," \"for shame deny that thou bear'st love to any\\nwho for thy self art so unprovident.\\ngrant if thou wilt, thou art beloved of many,\\nbut that thou none lov'st is most evident:\\nfor thou art so possessed with murd'rous hate,\\nthat 'gainst thy self thou stick'st not to conspire,\\nseeking that beauteous roof to ruinate\\nwhich to repair should be thy chief desire:\\no change thy thought, that i may change my mind,\\nshall hate be fairer lodged than gentle love?\\nbe as thy presence is gracious and kind,\\nor to thy self at least kind-hearted prove,\\n  make thee another self for love of me,\\n  that beauty still may live in thine or thee.*\"]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"aNHG9aBlw7Q6"},"source":["We need to encode the sonnets in vectors, and I will do this by representing each character with an integer in a list. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNzuq8TatE8J","executionInfo":{"status":"ok","timestamp":1615350356821,"user_tz":600,"elapsed":2007,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}},"outputId":"703645d6-d91b-484d-e9a1-c18cc419175e"},"source":["# create mapping of unique chars to integers\n","# first I need a new string with all the sonnets concatenated, so that the \n","# numbers above the sonnets are not included in the vocabulary. \n","raw_text = \"\"\n","for i in range(N):\n","  raw_text += sonnets[i]\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","int_to_char = dict((i, c) for i, c in enumerate(chars))\n","\n","# summarize the loaded data\n","n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total Characters: \", n_chars)\n","print(\"Total Vocab: \", n_vocab)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Total Characters:  94290\n","Total Vocab:  39\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0zBlUJRXzOvA"},"source":["For each sonnet, I want to include a training vector containing 40 characters, where the prediction target is the next character, up until the sonnet is expected to predict the terminal character. "]},{"cell_type":"code","metadata":{"id":"Ki4XK1TZyx8d","executionInfo":{"status":"ok","timestamp":1615350358090,"user_tz":600,"elapsed":3270,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}}},"source":["# We will predict each character using the preceding 40 characters.\n","seq_length = 40\n","\n","# How many entries are we going to have?\n","entries = 0\n","for i in range(N):\n","  entries += len(sonnets[i]) - seq_length\n","\n","# prepare the dataset of input to output pairs encoded as integers\n","dataX = np.zeros([entries, seq_length, 1], dtype=np.float32)\n","dataY = np.zeros(entries)\n","\n","entry = 0\n","for i in range(N):\n","  sonnet = sonnets[i]\n","  for j in range(seq_length, len(sonnet)):\n","    # record the character to be predicted\n","    dataY[entry] = char_to_int[sonnet[j]]\n","\n","    # record the training vector\n","    for k in range(40):\n","      dataX[entry, k, 0] = char_to_int[sonnet[j - seq_length + k]]\n","\n","    entry += 1\n","\n","# normalize the input vectors\n","X = dataX / (n_vocab - 1)\n","\n","# one hot encode the output variable\n","Y = np_utils.to_categorical(dataY)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Z3S3VDy3yO1"},"source":["Next I will define the recurrent neural net architecture as a model in Keras. As advised, I will use a single layer of 200 LSTM units, followed by an output layer with 39 units (one for each potential character) and softmax thresholds."]},{"cell_type":"code","metadata":{"id":"oY1QITp7-zWy","executionInfo":{"status":"ok","timestamp":1615350358092,"user_tz":600,"elapsed":3265,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}}},"source":["!mkdir checkpoints"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFggaHwQ3iy2","executionInfo":{"status":"ok","timestamp":1615350363738,"user_tz":600,"elapsed":8904,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}}},"source":["def get_model():\n","  \"Returns a keras recurrent neural net model with the desired architecture.\"\n","  # define the LSTM model\n","  model = Sequential()\n","  model.add(LSTM(200, input_shape=(seq_length, 1)))\n","  # Apply regularization. This greatly improved model performance. \n","  model.add(Dropout(0.1))\n","  # Add a softmax output layer.\n","  model.add(Dense(Y.shape[1], activation='softmax'))\n","  model.compile(loss='categorical_crossentropy', optimizer='adam')\n","  return model\n","\n","model = get_model()\n","# define the checkpoint\n","filepath=\"./checkpoints/weights-improvement-{epoch:02d}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=True, \n","                             save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Endq6f3yFvio"},"source":["First I will train with a validation set in order to determine the optimal number of epochs. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SY8QS-bSFtcX","executionInfo":{"status":"ok","timestamp":1615350490398,"user_tz":600,"elapsed":135558,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}},"outputId":"97ae0669-d584-4dfc-cfb0-77766ab65120"},"source":["# fit the model\n","N_test = 10000\n","X_train = X[N_test:]\n","Y_train = Y[N_test:]\n","X_test = X[:N_test]\n","Y_test = Y[:N_test]\n","model.fit(X_train, Y_train, epochs=25, batch_size=128, callbacks=callbacks_list,\n","          validation_data=(X_test, Y_test))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n","611/611 [==============================] - 37s 7ms/step - loss: 3.0372 - val_loss: 2.8692\n","\n","Epoch 00001: loss improved from inf to 2.98197, saving model to ./checkpoints/weights-improvement-01.hdf5\n","Epoch 2/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.8300 - val_loss: 2.7475\n","\n","Epoch 00002: loss improved from 2.98197 to 2.80282, saving model to ./checkpoints/weights-improvement-02.hdf5\n","Epoch 3/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.7325 - val_loss: 2.6967\n","\n","Epoch 00003: loss improved from 2.80282 to 2.72487, saving model to ./checkpoints/weights-improvement-03.hdf5\n","Epoch 4/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.6871 - val_loss: 2.6586\n","\n","Epoch 00004: loss improved from 2.72487 to 2.67493, saving model to ./checkpoints/weights-improvement-04.hdf5\n","Epoch 5/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.6423 - val_loss: 2.6251\n","\n","Epoch 00005: loss improved from 2.67493 to 2.63503, saving model to ./checkpoints/weights-improvement-05.hdf5\n","Epoch 6/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.6106 - val_loss: 2.6034\n","\n","Epoch 00006: loss improved from 2.63503 to 2.60394, saving model to ./checkpoints/weights-improvement-06.hdf5\n","Epoch 7/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.5865 - val_loss: 2.5859\n","\n","Epoch 00007: loss improved from 2.60394 to 2.57843, saving model to ./checkpoints/weights-improvement-07.hdf5\n","Epoch 8/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.5514 - val_loss: 2.5629\n","\n","Epoch 00008: loss improved from 2.57843 to 2.55534, saving model to ./checkpoints/weights-improvement-08.hdf5\n","Epoch 9/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.5332 - val_loss: 2.5619\n","\n","Epoch 00009: loss improved from 2.55534 to 2.53013, saving model to ./checkpoints/weights-improvement-09.hdf5\n","Epoch 10/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.5175 - val_loss: 2.5221\n","\n","Epoch 00010: loss improved from 2.53013 to 2.50760, saving model to ./checkpoints/weights-improvement-10.hdf5\n","Epoch 11/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.4805 - val_loss: 2.5082\n","\n","Epoch 00011: loss improved from 2.50760 to 2.48559, saving model to ./checkpoints/weights-improvement-11.hdf5\n","Epoch 12/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.4597 - val_loss: 2.5050\n","\n","Epoch 00012: loss improved from 2.48559 to 2.46127, saving model to ./checkpoints/weights-improvement-12.hdf5\n","Epoch 13/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.4375 - val_loss: 2.4886\n","\n","Epoch 00013: loss improved from 2.46127 to 2.44013, saving model to ./checkpoints/weights-improvement-13.hdf5\n","Epoch 14/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.4090 - val_loss: 2.4819\n","\n","Epoch 00014: loss improved from 2.44013 to 2.41589, saving model to ./checkpoints/weights-improvement-14.hdf5\n","Epoch 15/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.3853 - val_loss: 2.4758\n","\n","Epoch 00015: loss improved from 2.41589 to 2.39482, saving model to ./checkpoints/weights-improvement-15.hdf5\n","Epoch 16/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.3675 - val_loss: 2.4795\n","\n","Epoch 00016: loss improved from 2.39482 to 2.37082, saving model to ./checkpoints/weights-improvement-16.hdf5\n","Epoch 17/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.3460 - val_loss: 2.4614\n","\n","Epoch 00017: loss improved from 2.37082 to 2.34907, saving model to ./checkpoints/weights-improvement-17.hdf5\n","Epoch 18/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.3200 - val_loss: 2.4746\n","\n","Epoch 00018: loss improved from 2.34907 to 2.32211, saving model to ./checkpoints/weights-improvement-18.hdf5\n","Epoch 19/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.2979 - val_loss: 2.4741\n","\n","Epoch 00019: loss improved from 2.32211 to 2.29829, saving model to ./checkpoints/weights-improvement-19.hdf5\n","Epoch 20/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.2654 - val_loss: 2.4882\n","\n","Epoch 00020: loss improved from 2.29829 to 2.27534, saving model to ./checkpoints/weights-improvement-20.hdf5\n","Epoch 21/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.2380 - val_loss: 2.4937\n","\n","Epoch 00021: loss improved from 2.27534 to 2.24973, saving model to ./checkpoints/weights-improvement-21.hdf5\n","Epoch 22/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.2221 - val_loss: 2.5045\n","\n","Epoch 00022: loss improved from 2.24973 to 2.22332, saving model to ./checkpoints/weights-improvement-22.hdf5\n","Epoch 23/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.1867 - val_loss: 2.5147\n","\n","Epoch 00023: loss improved from 2.22332 to 2.19773, saving model to ./checkpoints/weights-improvement-23.hdf5\n","Epoch 24/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.1648 - val_loss: 2.5276\n","\n","Epoch 00024: loss improved from 2.19773 to 2.17486, saving model to ./checkpoints/weights-improvement-24.hdf5\n","Epoch 25/25\n","611/611 [==============================] - 4s 6ms/step - loss: 2.1360 - val_loss: 2.5343\n","\n","Epoch 00025: loss improved from 2.17486 to 2.15177, saving model to ./checkpoints/weights-improvement-25.hdf5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7faaba297290>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"Ffeazrz6F3o7"},"source":["We see that we get the lowest validation error around epoch 18. Thus, let's train on the full dataset for 18 epochs. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeH4PhUGF3UW","executionInfo":{"status":"ok","timestamp":1615350562339,"user_tz":600,"elapsed":207491,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}},"outputId":"54223efd-d7bb-4ad6-ffc8-771120e1fd22"},"source":["model = get_model()\n","model.fit(X, Y, epochs=18, batch_size=128, callbacks=callbacks_list)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch 1/18\n","689/689 [==============================] - 5s 6ms/step - loss: 3.0374\n","\n","Epoch 00001: loss did not improve from 2.15177\n","Epoch 2/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.8212\n","\n","Epoch 00002: loss did not improve from 2.15177\n","Epoch 3/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.7380\n","\n","Epoch 00003: loss did not improve from 2.15177\n","Epoch 4/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.6835\n","\n","Epoch 00004: loss did not improve from 2.15177\n","Epoch 5/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.6414\n","\n","Epoch 00005: loss did not improve from 2.15177\n","Epoch 6/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.6060\n","\n","Epoch 00006: loss did not improve from 2.15177\n","Epoch 7/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.5877\n","\n","Epoch 00007: loss did not improve from 2.15177\n","Epoch 8/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.5583\n","\n","Epoch 00008: loss did not improve from 2.15177\n","Epoch 9/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.5338\n","\n","Epoch 00009: loss did not improve from 2.15177\n","Epoch 10/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.5100\n","\n","Epoch 00010: loss did not improve from 2.15177\n","Epoch 11/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.4795\n","\n","Epoch 00011: loss did not improve from 2.15177\n","Epoch 12/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.4569\n","\n","Epoch 00012: loss did not improve from 2.15177\n","Epoch 13/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.4374\n","\n","Epoch 00013: loss did not improve from 2.15177\n","Epoch 14/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.4072\n","\n","Epoch 00014: loss did not improve from 2.15177\n","Epoch 15/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.3953\n","\n","Epoch 00015: loss did not improve from 2.15177\n","Epoch 16/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.3687\n","\n","Epoch 00016: loss did not improve from 2.15177\n","Epoch 17/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.3462\n","\n","Epoch 00017: loss did not improve from 2.15177\n","Epoch 18/18\n","689/689 [==============================] - 4s 6ms/step - loss: 2.3216\n","\n","Epoch 00018: loss did not improve from 2.15177\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7faab2c449d0>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"IsPZQtftLuDm"},"source":["Keras doesn't have an obvious method to sample the softmax output with a temperature parameter, so I have written my own function for this. "]},{"cell_type":"code","metadata":{"id":"MDTCt6APLsf2","executionInfo":{"status":"ok","timestamp":1615350562340,"user_tz":600,"elapsed":207486,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}}},"source":["def sample_softmax(probs, temp):\n","  # first we invert the exponentiation done in the softmax output\n","  probs = np.log(probs)\n","  # Next we divide by the temperature\n","  probs /= temp\n","  # We exponentiate again and normalize\n","  probs = np.exp(probs)\n","  probs /= np.sum(probs)\n","  # Finally, we draw a sample from the adjusted distribution. \n","  return np.random.choice(list(range(n_vocab)), p=probs)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pc9gXOqYNIEC"},"source":["We can now load the best performing model and generate poems, character at a time. "]},{"cell_type":"code","metadata":{"id":"6fJhJGqq8A2J","executionInfo":{"status":"ok","timestamp":1615351338585,"user_tz":600,"elapsed":680,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}}},"source":["def generate_poem(temp, seed= \"shall i compare thee to a summer's day?\\n\"):\n","  \"\"\"Returns a poem generated from a recurrent neural network\n","  trained on shakespeare's sonnets, seeded with a 40-character \n","  string 'seed' and sampled with a variance reflected in 'temp'.\"\"\"\n","  # load the network weights\n","  filename = './checkpoints/weights-improvement-18.hdf5'\n","  model.load_weights(filename)\n","  model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","  x = np.zeros([1, seq_length, 1])\n","  for i in range(seq_length):\n","    x[0, i, 0] = char_to_int[seed[i]]\n","\n","  # normalize the input\n","  x = x / (n_vocab - 1)\n","\n","  # generate characters. Hopefully the sonnet should terminate on its own, but\n","  # I set a max length of 2500 because that is several times the length of an \n","  # actual sonnet. \n","  poem = seed\n","  while poem[-1] != '*' and len(poem) < 2500:\n","    # predict the probability distribution on the next character\n","    prediction = model.predict(x, verbose=0)[0]\n","\n","    # sample the character from the distribution\n","    index = sample_softmax(prediction, temp)\n","\n","    # append the new character to the poem\n","    poem = poem + int_to_char[index]\n","\n","    # shift the input vector and append the predicted character to the end.\n","    x[0, 0:-1, 0] = x[0, 1:, 0]\n","    x[0, -1, 0] = index / (n_vocab - 1)\n","    \n","  return poem"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ISmHIDYFWodU"},"source":["Now I will generate poems using the desired temperatures. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPtpvrMs-p67","executionInfo":{"status":"ok","timestamp":1615351415275,"user_tz":600,"elapsed":76357,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}},"outputId":"03d12332-3382-4471-e764-c64078a683d2"},"source":["print(generate_poem(.25))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["shall i compare thee to a summer's day?\n","the pane tfe loeer of toes whet fear she sene,\n","whet thou mas thet whi heve and thete\n","\n"," aoe thet i ar for the wire   no shet ie more th thoe th thee whal she hr mene thee \n"," nhn toet a aoadn the lore tf thee mo tore \n","  mo so the love th thye the lore to thye\n","\n"," no soee i soml   no whet i ar ane then whech her she ken,\n","  and thes i woil a aaakt to thee whech whet i srene,\n","that i soel toetl oo tore and thin what s thet whel whet the sores tf thi seee \n"," no whet i soo she tored and thou miv so toewe,\n","  and thes i soo shee whoh the lase oo tees,\n","  and thet in thee  and thet thas lens of toaee,\n","  shet shiu d so teae in thes whech she maser so the sire,\n","  and thet iear ser the seas io thy soeme,\n","the lane the seat she more sf tiee whe hane th thee \n","  aed thet i soor sh thee whet fer the sire \n","  and goe thet sere that thnu whll pooesse that whet i soo mene the lores of thet shi helt,\n","th thet io thee a aoank so tiat is soeete thet sille thet sers \n","aod thet io whet i thi kese aad   and thet ie teit io the worll tf toee \n","oo thet i ar aol men thet heart the haart oh the sire \n","nhs sees i soene oo tore to thy sele tore,\n","  fn the wirt dearte tiat io the rire dod thete \n"," nin thet i ar ail mev siet fear so tiee,\n","  and thet io the badr mo thes in the rirees \n","and the fase he rhi lreerer tfet fiares so the sire,\n","  and then i soine the whrl whoh the wert sioe,\n","that thou art the leve the pores sf thee \n","  and thet i soene the whrl  and thet ie rore the lase,\n","  and thet the whrt sh thae whet ies thet doe thet fned shee \n","an mo toee a soreng oo the sire,\n","  and thet ies soee   yhet shet i soel th the world to toete,\n","thet theu me tooe and and the paan a soridt of the sine,\n","  and thet i soirt sh thae whal i shel i sha lore thel the hess,\n","fnd thet io the sorene to meve the laar,\n","whet thou me too so tou mo toae to thee siee whrt seee,\n","thet mo the laer of toite to toet whe hane,\n","thet mos the lene and then i soin whel the \n","aod ave the maane th the whrl soors,\n","oo thet the well i shey the sirs oo toeet,\n","  and the aaarte the mooe and hore if shre\n","\n"," fo mo ho the sere   no seat io this  and thet io the whrh tiee \n","  aid thet theu ters  and thes i soiel mo thee \n"," nin thet i ar ael and thes thi hes so seet,\n","  and thet i sooe thes whll a aoadd the tees,\n","  the worr   nise a aelse thi heart th thee in mo here tored that iene a soren if   soen thet ie thie i soin a doatter dea\n","thi sent aesire the pore mo thy soest,\n","a dout the wirl  and then i ao mo whet,\n","thet io th\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5UZjwkFx-sTU","executionInfo":{"status":"ok","timestamp":1615351455549,"user_tz":600,"elapsed":115349,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}},"outputId":"bbefab27-7ddb-4a58-ace9-bff31bca96fb"},"source":["print(generate_poem(.75))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["shall i compare thee to a summer's day?\n","to caleyt soamd ane are sho buete nn mo.\n","thr shyele the iiasee shar tho ruueaissets ardm,\n","ohs shot noo shift enstgr thd pore si thy danrels,\n","ant yhis h sas mea aod the peekter or toewi,\n"," nt waal any thai mh mems to the beler \n"," nocg fy ln eore hnr teyued cnd tianr foeae\n","aannere, thet hosd tem poiets, ard thy shr grise oh,toe a torehne sooue,\n","thas hhaly io then what weo hes seeee \n"," oef avu eu liah me aod mo thmt rt wout,\n"," nfs haad sias whtt srg\n","theie shtuel soodd.\n","woee mi thet worrd shy soaki ro breas \n","aot bead i cey ailwc dedat titem hore uoerd if iin \n","sheel tiu toeotr oate do the wirldr trats,\n","when hen thy brnl'else hane ho iaatt no alids \n","aeti tterne to mo ooves sy toie wes eeast,\n","who iav thn gens aet blte woet thou st \n","ert mr io c rai pam strrte,faro.\n","tfo asr dnr oy fesi ot pafs sie ioenn,\n","tnnn me theu feccr ffn saet bytere tooke \n"," nhe pis boset sho shie aatse bads ie shs bruhns,deg, tien iaot lo my srigte'tn it gemt sooe,\n","aod yhu ans mom tho seme in what i tbye to daae \n"," yhe tht hm teu ii mevps why men sheal i pnnert,\n","anm whun ant tierei seo mu self a poredsg  eadr,nt torteb\n","fnrei,tyer sf tie mlnrti bnweu of move thee vhnl hlnprger \n","and hs mhs seat ir liv s yhen i iave toen dti yhet i woeak ror shot \n","thy sild ayae mase tie sirt hxpr aami ther secl fft sian aasdn,d it me mhr*\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JR319feqCqNZ","executionInfo":{"status":"ok","timestamp":1615351456932,"user_tz":600,"elapsed":116252,"user":{"displayName":"John Heath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibRkrmM8lI0my0UktYni8u1ITuOzxilN0Xr2m9MA=s64","userId":"13426123499539180141"}},"outputId":"2048e0fd-a382-432d-e971-d9ed205d8623"},"source":["print(generate_poem(1.5))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["shall i compare thee to a summer's day?\n","fol dnr mft govp'add lo iyrbedc mao\n","*\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uMknHc0ub2n0"},"source":[""],"execution_count":null,"outputs":[]}]}